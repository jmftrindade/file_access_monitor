{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Events of interest from raw instrumentation logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('lineage_edges_1000.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build a Neo4j lineage graph from instrumentation logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lineage_graph_cosmos as lg\n",
    "\n",
    "graph = lg.build_lineage_graph('lineage_edges_1000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Custom visualization of lineage graph using Vis.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scripts.vis import draw\n",
    "\n",
    "options = {\"Dataset\": \"name\", \"Job\": \"uid\"}\n",
    "draw(graph, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Query Neo4j temporal lineage graph using Cypher\n",
    "\n",
    "## Query results as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check query.\n",
    "query = \"\"\"\n",
    "    MATCH (n)-[r]->(m)\n",
    "    RETURN id(n) AS source_id,\n",
    "           id(r) AS rel_id,\n",
    "           id(m) AS target_id\n",
    "\"\"\"\n",
    "\n",
    "results_df = pd.DataFrame(graph.data(query))\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Copy detection as a reachability query between 2 content similar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    MATCH (n:Dataset)\n",
    "    WHERE (n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'\n",
    "    OR n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv')\n",
    "    RETURN id(n) as node_id\n",
    "\"\"\"\n",
    "node_ids = graph.data(query)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(a)-[r*]->(b)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from a to b:\\n' % len(paths))\n",
    "print(paths)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(b)-[r*]->(a)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from b to a:\\n' % len(paths))\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Copy detection as a reachability query between 2 content similar datasets (monotonically increasing edge timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    MATCH (n:Dataset)\n",
    "    WHERE (n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'\n",
    "    OR n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv')\n",
    "    RETURN id(n) as node_id\n",
    "\"\"\"\n",
    "node_ids = graph.data(query)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(a)-[r*]->(b)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    WHERE all(r2 in relationships(p)\n",
    "              where r2.timestamp>=r1.timestamp)    \n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from a to b with monotonically increasing timestamps:\\n' % len(paths))\n",
    "print(paths)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(b)-[r*]->(a)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    WHERE all(r2 in relationships(p)\n",
    "              where r2.timestamp>=r1.timestamp)    \n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from b to a with monotonically increasing timestamps:\\n' % len(paths))\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext cypher\n",
    "import networkx as nx\n",
    "import operator\n",
    "%matplotlib inline\n",
    "\n",
    "results = %cypher MATCH p = (a)-[r*]->(b) RETURN p\n",
    "\n",
    "# Networkx graph.\n",
    "g = results.get_graph()\n",
    "\n",
    "#nx.draw(g)\n",
    "#g.nodes(data=True)\n",
    "\n",
    "# Print nodes so that we can see their original ids and properties.\n",
    "print(\"\\nOriginal Nodes:\")\n",
    "print(g.nodes(data=True))\n",
    "\n",
    "# Node weights for personalized pagerank.\n",
    "personalize = {}\n",
    "for node, data in g.nodes(data=True):\n",
    "    if 'Dataset' in data['labels']:\n",
    "        personalize[node] = 1\n",
    "    elif 'Job' in data['labels']:\n",
    "        # Let's weight each edge by the amount of CPU consumed.\n",
    "        personalize[node] = data['cpu']\n",
    "    print(\"node=%s, data=%s\" % (node, data))\n",
    "\n",
    "print(\"\\nPersonalize Vector:\")\n",
    "print(personalize)\n",
    "\n",
    "# Transformation from MultiDigraph to Graph for Pagerank calculation.\n",
    "H = nx.Graph()\n",
    "for src, dst, edge in g.edges(data=True):\n",
    "    # Let's weight each edge by the amount of bytes read / written.\n",
    "    w = edge['data_bytes']\n",
    "    if H.has_edge(src, dst):\n",
    "        H[src][dst]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(src, dst, weight=w)\n",
    "\n",
    "#print(\"\\nPageRank:\")\n",
    "#print(nx.pagerank(H))\n",
    "\n",
    "print(\"\\nPersonalized PageRank:\")\n",
    "ranks = nx.pagerank(H, personalization=personalize)\n",
    "sorted_ranks = sorted(ranks.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(sorted_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Lineage Temporal Graph (Cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load 20000 rows into a neo4j graph: 17109 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from py2neo import Graph\n",
    "\n",
    "# Create new graph from scratch, deleting all previous graphs from the DB.\n",
    "graph = Graph()\n",
    "graph.delete_all()\n",
    "\n",
    "# Load CSV into a neo4j graph.\n",
    "pwd = os.getcwd()\n",
    "num_rows = 20000  # Edit this to pick the correct file.\n",
    "filename = 'file:///%s/lineage_edges_%s.csv' % (pwd, num_rows)\n",
    "query = \"\"\"\n",
    "USING PERIODIC COMMIT \n",
    "LOAD CSV WITH HEADERS FROM '%s'\n",
    "  AS line\n",
    "  FIELDTERMINATOR ','\n",
    "MERGE (job:Job { id: line.Job, name: line.Job, uid: line.Job })\n",
    "MERGE (dataset:Dataset { id: line.Dataset, name: line.Dataset })\n",
    "FOREACH(t in CASE WHEN line.Action='read' THEN [1] ELSE [] END |\n",
    "    CREATE (dataset)-[:IS_READ_BY {time: line.StartTime}]->(job))\n",
    "FOREACH(t in CASE WHEN line.Action='write' THEN [1] ELSE [] END |\n",
    "    CREATE (job)-[:WRITES_TO {time: line.StartTime}]->(dataset))\n",
    "\"\"\" % filename\n",
    "\n",
    "t0 = int(round(time.time() * 1000))\n",
    "graph.data(query)\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to load %s rows into a neo4j graph: %s ms' % (num_rows, (t1 - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"figure/graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1077a0fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.vis_cosmos import draw\n",
    "\n",
    "graph = Graph()\n",
    "res = graph.data(filter_query)\n",
    "options = {\"Dataset\": \"name\", \"Job\": \"uid\"}\n",
    "draw(graph, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helios Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from py2neo import Graph\n",
    "\n",
    "# Create new graph from scratch, deleting all previous graphs from the DB.\n",
    "graph = Graph()\n",
    "graph.delete_all()\n",
    "\n",
    "# Load Helios queries CSV into a neo4j graph.\n",
    "pwd = os.getcwd()\n",
    "filename = 'file:///%s/helios_queries.csv' % pwd\n",
    "query = \"\"\"\n",
    "USING PERIODIC COMMIT \n",
    "LOAD CSV WITH HEADERS FROM '%s'\n",
    "  AS line\n",
    "  FIELDTERMINATOR ','\n",
    "MERGE (query:Query { id: line.scriptindexqueryindex, name: line.queryname })\n",
    "MERGE (table:Table { id: line.referencedtables, name: line.referencedtables})\n",
    "FOREACH(t in CASE WHEN line.referencedtables<>'' THEN [1] ELSE [] END |\n",
    "    CREATE (query)-[:REFERENCES]->(table))\n",
    "\"\"\" % filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagerank and other algos using networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "num_hops = 3\n",
    "query = \"\"\"\n",
    "    MATCH p=(n)-[r:SENT_EMAIL_TO*%s]->(m)\n",
    "    RETURN p\n",
    "\"\"\" % num_hops\n",
    "\n",
    "t0 = int(round(time.time() * 1000))\n",
    "motifs = graph.data(query)\n",
    "t1 = int(round(time.time() * 1000))\n",
    "\n",
    "print('Time to find %s-hop motifs: %s ms' % (num_hops, (t1 - t0)))\n",
    "#print('%s-hop motifs: %s' % (motifs, num_hops))\n",
    "\n",
    "%load_ext cypher\n",
    "import networkx as nx\n",
    "import operator\n",
    "%matplotlib inline\n",
    "\n",
    "# Networkx graph.\n",
    "t0 = int(round(time.time() * 1000))\n",
    "results = %cypher MATCH p = (a)-[r]->(b) RETURN p\n",
    "g = results.get_graph()\n",
    "#nx.draw(g)\n",
    "g.nodes(data=True)\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to load it into a networkx graph: %s ms' % (t1 - t0))\n",
    "\n",
    "t0 = int(round(time.time() * 1000))\n",
    "#print('In-degree: %s' % g.in_degree())\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to compute in-degree: %s ms' % (t1 - t0))\n",
    "\n",
    "t0 = int(round(time.time() * 1000))       \n",
    "centrality = nx.betweenness_centrality(g)\n",
    "#print('Centrality: %s' % centrality)\n",
    "t1 = int(round(time.time() * 1000))       \n",
    "print('Time to compute betweeness centrality: %s ms' % (t1 - t0))\n",
    "\n",
    "\n",
    "# Print nodes so that we can see their original ids and properties.\n",
    "#print(\"\\nOriginal Nodes:\")\n",
    "#print(g.nodes(data=True))\n",
    "\n",
    "# Node weights for personalized pagerank.\n",
    "t0 = int(round(time.time() * 1000))\n",
    "personalize = {}\n",
    "for node, data in g.nodes(data=True):\n",
    "    if 'Person' in data['labels']:\n",
    "        personalize[node] = 1\n",
    "    #print(\"node=%s, data=%s\" % (node, data))\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to create PageRank personalization vector: %s ms' % (t1 - t0))\n",
    "\n",
    "    \n",
    "#print(\"\\nPersonalize Vector:\")\n",
    "#print(personalize)\n",
    "\n",
    "# Transformation from MultiDigraph to Graph for Pagerank calculation.\n",
    "t0 = int(round(time.time() * 1000))\n",
    "H = nx.Graph()\n",
    "for src, dst, edge in g.edges(data=True):\n",
    "    # Let's weight each edge by the amount of bytes read / written.\n",
    "    w = int(edge['time'])\n",
    "    if H.has_edge(src, dst):\n",
    "        H[src][dst]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(src, dst, weight=w)\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to convert from MultiDigraph to Graph for PageRank calculation: %s ms' % (t1 - t0))\n",
    "\n",
    "\n",
    "t0 = int(round(time.time() * 1000))       \n",
    "ranks = nx.pagerank(H, personalization=personalize)\n",
    "sorted_ranks = sorted(ranks.items(), key=operator.itemgetter(1), reverse=True)\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to compute Pagerank: %s ms' % (t1 - t0))\n",
    "\n",
    "#print(\"\\nPersonalized PageRank:\")\n",
    "#print(sorted_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
