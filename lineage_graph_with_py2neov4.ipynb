{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Events of interest from raw instrumentation logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logs_df = pd.read_csv('loggedfs_events_pagerank_asymmetrical.log')\n",
    "cols = [col for col in logs_df.columns if col in ['Action','Time','Path','PID','PPID','UID','Command Line']]\n",
    "df = logs_df[cols]\n",
    "df[(df['Action'] == 'read') | (df['Action'] == 'write')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build a Neo4j lineage graph from instrumentation logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lineage_graph as lg\n",
    "\n",
    "graph = lg.build_lineage_graph('loggedfs_events_pagerank_asymmetrical.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Custom visualization of lineage graph using Vis.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scripts.vis import draw\n",
    "\n",
    "options = {\"Dataset\": \"name\", \"Job\": \"uid\"}\n",
    "draw(graph, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Query Neo4j temporal lineage graph using Cypher\n",
    "\n",
    "## Query results as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check query.\n",
    "query = \"\"\"\n",
    "    MATCH (n)-[r]->(m)\n",
    "    RETURN id(n) AS source_id,\n",
    "           id(r) AS rel_id,\n",
    "           id(m) AS target_id\n",
    "\"\"\"\n",
    "\n",
    "results_df = pd.DataFrame(graph.data(query))\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Copy detection as a reachability query between 2 content similar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    MATCH (n:Dataset)\n",
    "    WHERE (n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'\n",
    "    OR n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv')\n",
    "    RETURN id(n) as node_id\n",
    "\"\"\"\n",
    "node_ids = graph.data(query)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(a)-[r*]->(b)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from a to b:\\n' % len(paths))\n",
    "print(paths)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(b)-[r*]->(a)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from b to a:\\n' % len(paths))\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Copy detection as a reachability query between 2 content similar datasets (monotonically increasing edge timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    MATCH (n:Dataset)\n",
    "    WHERE (n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'\n",
    "    OR n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv')\n",
    "    RETURN id(n) as node_id\n",
    "\"\"\"\n",
    "node_ids = graph.data(query)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(a)-[r*]->(b)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    WHERE all(r2 in relationships(p)\n",
    "              where r2.timestamp>=r1.timestamp)    \n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from a to b with monotonically increasing timestamps:\\n' % len(paths))\n",
    "print(paths)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(b)-[r*]->(a)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    WHERE all(r2 in relationships(p)\n",
    "              where r2.timestamp>=r1.timestamp)    \n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from b to a with monotonically increasing timestamps:\\n' % len(paths))\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Aggregate Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    START from=node(*)\n",
    "    MATCH p=(from)-->(to)\n",
    "    WITH from as from, to as to, count(p) as paths\n",
    "    WHERE paths >= 1\n",
    "    RETURN to,paths\n",
    "\"\"\"\n",
    "\n",
    "paths = graph.data(query)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext cypher\n",
    "import networkx as nx\n",
    "import operator\n",
    "%matplotlib inline\n",
    "\n",
    "results = %cypher MATCH p = (a)-[r*]->(b) RETURN p\n",
    "\n",
    "# Networkx graph.\n",
    "g = results.get_graph()\n",
    "\n",
    "#nx.draw(g)\n",
    "#g.nodes(data=True)\n",
    "\n",
    "# Print nodes so that we can see their original ids and properties.\n",
    "print(\"\\nOriginal Nodes:\")\n",
    "print(g.nodes(data=True))\n",
    "\n",
    "# Node weights for personalized pagerank.\n",
    "personalize = {}\n",
    "for node, data in g.nodes(data=True):\n",
    "    if 'Dataset' in data['labels']:\n",
    "        personalize[node] = 1\n",
    "    elif 'Job' in data['labels']:\n",
    "        # Let's weight each edge by the amount of CPU consumed.\n",
    "        personalize[node] = data['cpu']\n",
    "    print(\"node=%s, data=%s\" % (node, data))\n",
    "\n",
    "print(\"\\nPersonalize Vector:\")\n",
    "print(personalize)\n",
    "\n",
    "# Transformation from MultiDigraph to Graph for Pagerank calculation.\n",
    "H = nx.Graph()\n",
    "for src, dst, edge in g.edges(data=True):\n",
    "    # Let's weight each edge by the amount of bytes read / written.\n",
    "    w = edge['data_bytes']\n",
    "    if H.has_edge(src, dst):\n",
    "        H[src][dst]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(src, dst, weight=w)\n",
    "\n",
    "#print(\"\\nPageRank:\")\n",
    "#print(nx.pagerank(H))\n",
    "\n",
    "print(\"\\nPersonalized PageRank:\")\n",
    "ranks = nx.pagerank(H, personalization=personalize)\n",
    "sorted_ranks = sorted(ranks.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(sorted_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Snap Temporal Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load 10000 rows into a neo4j graph: 12245 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from py2neo import Graph\n",
    "\n",
    "# Cleanup all existing graphs.\n",
    "#query = \"\"\"\n",
    "#MATCH (n)\n",
    "#WITH n limit 100000\n",
    "#DETACH DELETE n;  \n",
    "#\"\"\"\n",
    "#graph = Graph()\n",
    "#graph.data(query)\n",
    "\n",
    "pwd = os.getcwd()\n",
    "num_rows = 10000\n",
    "filename = 'file:///%s/snap_temporal_graph_dataset/email-Eu-core-Dept3-%s.txt' % (pwd, num_rows)\n",
    "\n",
    "# Load CSV into a neo4j graph.\n",
    "query = \"\"\"\n",
    "USING PERIODIC COMMIT \n",
    "LOAD CSV WITH HEADERS FROM '%s'\n",
    "  AS line\n",
    "  FIELDTERMINATOR ' '\n",
    "MERGE (src:Person { id: line.src })\n",
    "MERGE (dst:Person { id: line.dst })\n",
    "CREATE (src)-[:SENT_EMAIL_TO {time: line.time}]->(dst)\n",
    "\"\"\" % filename\n",
    "graph = Graph()\n",
    "graph.delete_all()\n",
    "\n",
    "t0 = int(round(time.time() * 1000))\n",
    "graph.data(query)\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to load %s rows into a neo4j graph: %s ms' % (num_rows, (t1 - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fetch temporal graph and make it available as a Pandas dataframe: 660 ms\n",
      "The cypher extension is already loaded. To reload it, use:\n",
      "  %reload_ext cypher\n",
      "10000 rows affected.\n",
      "Time to load it into a networkx graph: 2281 ms\n",
      "Time to compute in-degree: 0 ms\n",
      "Time to compute betweeness centrality: 57 ms\n",
      "Time to create PageRank personalization vector: 0 ms\n",
      "Time to convert from MultiDigraph to Graph for PageRank calculation: 37 ms\n",
      "Time to compute Pagerank: 56 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "query = \"\"\"\n",
    "    MATCH (n)-[r]->(m)\n",
    "    RETURN id(n) AS source_id,\n",
    "           id(r) AS rel_id,\n",
    "           id(m) AS target_id\n",
    "\"\"\"\n",
    "\n",
    "t0 = int(round(time.time() * 1000))\n",
    "results_df = pd.DataFrame(graph.data(query))\n",
    "t1 = int(round(time.time() * 1000))\n",
    "\n",
    "print('Time to fetch temporal graph and make it available as a Pandas dataframe: %s ms' % (t1 - t0))\n",
    "\n",
    "%load_ext cypher\n",
    "import networkx as nx\n",
    "import operator\n",
    "%matplotlib inline\n",
    "\n",
    "# Networkx graph.\n",
    "t0 = int(round(time.time() * 1000))\n",
    "results = %cypher MATCH p = (a)-[r]->(b) RETURN p\n",
    "g = results.get_graph()\n",
    "#nx.draw(g)\n",
    "g.nodes(data=True)\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to load it into a networkx graph: %s ms' % (t1 - t0))\n",
    "\n",
    "t0 = int(round(time.time() * 1000))\n",
    "#print('In-degree: %s' % g.in_degree())\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to compute in-degree: %s ms' % (t1 - t0))\n",
    "\n",
    "t0 = int(round(time.time() * 1000))       \n",
    "centrality = nx.betweenness_centrality(g)\n",
    "#print('Centrality: %s' % centrality)\n",
    "t1 = int(round(time.time() * 1000))       \n",
    "print('Time to compute betweeness centrality: %s ms' % (t1 - t0))\n",
    "\n",
    "\n",
    "# Print nodes so that we can see their original ids and properties.\n",
    "#print(\"\\nOriginal Nodes:\")\n",
    "#print(g.nodes(data=True))\n",
    "\n",
    "# Node weights for personalized pagerank.\n",
    "t0 = int(round(time.time() * 1000))\n",
    "personalize = {}\n",
    "for node, data in g.nodes(data=True):\n",
    "    if 'Person' in data['labels']:\n",
    "        personalize[node] = 1\n",
    "    #print(\"node=%s, data=%s\" % (node, data))\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to create PageRank personalization vector: %s ms' % (t1 - t0))\n",
    "\n",
    "    \n",
    "#print(\"\\nPersonalize Vector:\")\n",
    "#print(personalize)\n",
    "\n",
    "# Transformation from MultiDigraph to Graph for Pagerank calculation.\n",
    "t0 = int(round(time.time() * 1000))\n",
    "H = nx.Graph()\n",
    "for src, dst, edge in g.edges(data=True):\n",
    "    # Let's weight each edge by the amount of bytes read / written.\n",
    "    w = int(edge['time'])\n",
    "    if H.has_edge(src, dst):\n",
    "        H[src][dst]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(src, dst, weight=w)\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to convert from MultiDigraph to Graph for PageRank calculation: %s ms' % (t1 - t0))\n",
    "\n",
    "\n",
    "t0 = int(round(time.time() * 1000))       \n",
    "ranks = nx.pagerank(H, personalization=personalize)\n",
    "sorted_ranks = sorted(ranks.items(), key=operator.itemgetter(1), reverse=True)\n",
    "t1 = int(round(time.time() * 1000))\n",
    "print('Time to compute Pagerank: %s ms' % (t1 - t0))\n",
    "\n",
    "#print(\"\\nPersonalized PageRank:\")\n",
    "#print(sorted_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
