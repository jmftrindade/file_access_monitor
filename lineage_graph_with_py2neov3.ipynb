{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events of interest from raw instrumentation logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Action</th>\n",
       "      <th>Path</th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>PPID</th>\n",
       "      <th>Command Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1490369977685</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>3013</td>\n",
       "      <td>3012</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out2.csv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1490369977690</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>3013</td>\n",
       "      <td>3012</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out2.csv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1490369977692</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>3013</td>\n",
       "      <td>3012</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out2.csv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1490369977696</td>\n",
       "      <td>write</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>3013</td>\n",
       "      <td>3012</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out2.csv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1490369979040</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>3020</td>\n",
       "      <td>3012</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out2.csv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1490369979042</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>3020</td>\n",
       "      <td>3012</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out2.csv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1490369979044</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>3020</td>\n",
       "      <td>3012</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out2.csv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1490369979049</td>\n",
       "      <td>write</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>3020</td>\n",
       "      <td>3012</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out2.csv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time Action                                               Path  \\\n",
       "1   1490369977685   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "3   1490369977690   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "5   1490369977692   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "8   1490369977696  write  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "10  1490369979040   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "12  1490369979042   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "14  1490369979044   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "17  1490369979049  write  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "\n",
       "     UID   PID  PPID                                       Command Line  \n",
       "1   1000  3013  3012  python concat_csvs.py in1.csv in2.csv out2.csv...  \n",
       "3   1000  3013  3012  python concat_csvs.py in1.csv in2.csv out2.csv...  \n",
       "5   1000  3013  3012  python concat_csvs.py in1.csv in2.csv out2.csv...  \n",
       "8   1000  3013  3012  python concat_csvs.py in1.csv in2.csv out2.csv...  \n",
       "10  1000  3020  3012  python concat_csvs.py in1.csv in2.csv out2.csv...  \n",
       "12  1000  3020  3012  python concat_csvs.py in1.csv in2.csv out2.csv...  \n",
       "14  1000  3020  3012  python concat_csvs.py in1.csv in2.csv out2.csv...  \n",
       "17  1000  3020  3012  python concat_csvs.py in1.csv in2.csv out2.csv...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logs_df = pd.read_csv('loggedfs_events_pagerank.log')\n",
    "cols = [col for col in logs_df.columns if col in ['Action','Time','Path','PID','PPID','UID','Command Line']]\n",
    "df = logs_df[cols]\n",
    "df[(df['Action'] == 'read') | (df['Action'] == 'write')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Neo4j lineage graph from instrumentation logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lineage_graph as lg\n",
    "\n",
    "graph = lg.build_lineage_graph('loggedfs_events_pagerank.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom visualization of lineage graph using Vis.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"figure/graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x113421f60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.vis import draw\n",
    "\n",
    "options = {\"Dataset\": \"name\", \"Job\": \"uid\"}\n",
    "draw(graph, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Neo4j temporal lineage graph using Cypher\n",
    "\n",
    "## Query results as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check query.\n",
    "query = \"\"\"\n",
    "    MATCH (n)-[r]->(m)\n",
    "    RETURN id(n) AS source_id,\n",
    "           id(r) AS rel_id,\n",
    "           id(m) AS target_id\n",
    "\"\"\"\n",
    "\n",
    "results_df = pd.DataFrame(graph.data(query))\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy detection as a reachability query between 2 content similar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    MATCH (n:Dataset)\n",
    "    WHERE (n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'\n",
    "    OR n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv')\n",
    "    RETURN id(n) as node_id\n",
    "\"\"\"\n",
    "node_ids = graph.data(query)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(a)-[r*]->(b)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from a to b:\\n' % len(paths))\n",
    "print(paths)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(b)-[r*]->(a)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from b to a:\\n' % len(paths))\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy detection as a reachability query between 2 content similar datasets (monotonically increasing edge timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    MATCH (n:Dataset)\n",
    "    WHERE (n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'\n",
    "    OR n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv')\n",
    "    RETURN id(n) as node_id\n",
    "\"\"\"\n",
    "node_ids = graph.data(query)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(a)-[r*]->(b)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    WHERE all(r2 in relationships(p)\n",
    "              where r2.timestamp>=r1.timestamp)    \n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from a to b with monotonically increasing timestamps:\\n' % len(paths))\n",
    "print(paths)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(b)-[r*]->(a)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    WHERE all(r2 in relationships(p)\n",
    "              where r2.timestamp>=r1.timestamp)    \n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from b to a with monotonically increasing timestamps:\\n' % len(paths))\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Aggregate Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    START from=node(*)\n",
    "    MATCH p=(from)-->(to)\n",
    "    WITH from as from, to as to, count(p) as paths\n",
    "    WHERE paths >= 1\n",
    "    RETURN to,paths\n",
    "\"\"\"\n",
    "\n",
    "paths = graph.data(query)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cypher extension is already loaded. To reload it, use:\n",
      "  %reload_ext cypher\n",
      "14 rows affected.\n",
      "\n",
      "Original Nodes:\n",
      "[('109', {'uid': 'PID=3013,CLI=python concat_csvs.py in1.csv in2.csv out2.csv out3.csv ', 'labels': ['Job'], 'name': 'python concat_csvs.py in1.csv in2.csv out2.csv out3.csv ', 'cpu': 85.04303955809714, 'pid': 3013}), ('108', {'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'}), ('113', {'uid': 'PID=3020,CLI=python concat_csvs.py in1.csv in2.csv out2.csv out4.csv ', 'labels': ['Job'], 'name': 'python concat_csvs.py in1.csv in2.csv out2.csv out4.csv ', 'cpu': 54.478126156204524, 'pid': 3020}), ('110', {'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/in2.csv'}), ('112', {'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out3.csv'}), ('114', {'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv'}), ('111', {'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out2.csv'})]\n",
      "node=109, data={'uid': 'PID=3013,CLI=python concat_csvs.py in1.csv in2.csv out2.csv out3.csv ', 'labels': ['Job'], 'name': 'python concat_csvs.py in1.csv in2.csv out2.csv out3.csv ', 'cpu': 85.04303955809714, 'pid': 3013}\n",
      "node=108, data={'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'}\n",
      "node=113, data={'uid': 'PID=3020,CLI=python concat_csvs.py in1.csv in2.csv out2.csv out4.csv ', 'labels': ['Job'], 'name': 'python concat_csvs.py in1.csv in2.csv out2.csv out4.csv ', 'cpu': 54.478126156204524, 'pid': 3020}\n",
      "node=110, data={'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/in2.csv'}\n",
      "node=112, data={'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out3.csv'}\n",
      "node=114, data={'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv'}\n",
      "node=111, data={'labels': ['Dataset'], 'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out2.csv'}\n",
      "\n",
      "Personalize Vector:\n",
      "{'109': 85.04303955809714, '108': 0, '111': 0, '110': 0, '112': 0, '114': 0, '113': 54.478126156204524}\n",
      "\n",
      "PageRank:\n",
      "{'109': 0.2413134344543876, '108': 0.12398613991566335, '111': 0.12398613991566335, '110': 0.12398613991566335, '114': 0.07270735567211739, '112': 0.07270735567211739, '113': 0.24131343445438758}\n",
      "\n",
      "Personalized PageRank:\n",
      "{'109': 0.29032168935664343, '108': 0.11486524609277028, '113': 0.2502173262722752, '110': 0.11486524609277028, '114': 0.05317153446867102, '112': 0.061693711624099264, '111': 0.11486524609277028}\n"
     ]
    }
   ],
   "source": [
    "%load_ext cypher\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "\n",
    "results = %cypher MATCH p = (a)-[r*]->(b) RETURN p\n",
    "\n",
    "# Networkx graph.\n",
    "g = results.get_graph()\n",
    "\n",
    "#nx.draw(g)\n",
    "#g.nodes(data=True)\n",
    "\n",
    "# Print nodes so that we can see their original ids and properties.\n",
    "print(\"\\nOriginal Nodes:\")\n",
    "print(g.nodes(data=True))\n",
    "\n",
    "# Node weights for personalized pagerank.\n",
    "personalize = {}\n",
    "for node, data in g.nodes(data=True):\n",
    "    if 'Dataset' in data['labels']:\n",
    "        personalize[node] = 0\n",
    "    elif 'Job' in data['labels']:\n",
    "        # Let's weight each edge by the amount of CPU consumed.\n",
    "        personalize[node] = data['cpu']\n",
    "    print(\"node=%s, data=%s\" % (node, data))\n",
    "\n",
    "print(\"\\nPersonalize Vector:\")\n",
    "print(personalize)\n",
    "\n",
    "# Transformation from MultiDigraph to Graph for Pagerank calculation.\n",
    "H = nx.Graph()\n",
    "for src, dst, edge in g.edges(data=True):\n",
    "    # Let's weight each edge by the amount of bytes read / written.\n",
    "    w = edge['data_bytes']\n",
    "    if H.has_edge(src, dst):\n",
    "        H[src][dst]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(src, dst, weight=w)\n",
    "\n",
    "print(\"\\nPageRank:\")\n",
    "print(nx.pagerank(H))\n",
    "print(\"\\nPersonalized PageRank:\")\n",
    "print(nx.pagerank(H, personalization=personalize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
