{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events of interest from raw instrumentation logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Action</th>\n",
       "      <th>Path</th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>PPID</th>\n",
       "      <th>Command Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1486075407888</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>17110</td>\n",
       "      <td>17109</td>\n",
       "      <td>cp in1.csv out1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1486075407888</td>\n",
       "      <td>write</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>17110</td>\n",
       "      <td>17109</td>\n",
       "      <td>cp in1.csv out1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1486075408892</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>17112</td>\n",
       "      <td>17109</td>\n",
       "      <td>cat in1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1486075408892</td>\n",
       "      <td>write</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>17112</td>\n",
       "      <td>17109</td>\n",
       "      <td>cat in1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1486075410214</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>17114</td>\n",
       "      <td>17109</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1486075410215</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>17114</td>\n",
       "      <td>17109</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1486075410219</td>\n",
       "      <td>write</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>17114</td>\n",
       "      <td>17109</td>\n",
       "      <td>python concat_csvs.py in1.csv in2.csv out3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1486075411282</td>\n",
       "      <td>read</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>17121</td>\n",
       "      <td>17109</td>\n",
       "      <td>cat out3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1486075411283</td>\n",
       "      <td>write</td>\n",
       "      <td>/home/lubuntu/src/file_access_monitor/test_wor...</td>\n",
       "      <td>1000</td>\n",
       "      <td>17121</td>\n",
       "      <td>17109</td>\n",
       "      <td>cat out3.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time Action                                               Path  \\\n",
       "2   1486075407888   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "4   1486075407888  write  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "7   1486075408892   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "9   1486075408892  write  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "11  1486075410214   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "13  1486075410215   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "16  1486075410219  write  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "19  1486075411282   read  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "21  1486075411283  write  /home/lubuntu/src/file_access_monitor/test_wor...   \n",
       "\n",
       "     UID    PID   PPID                                     Command Line  \n",
       "2   1000  17110  17109                             cp in1.csv out1.csv   \n",
       "4   1000  17110  17109                             cp in1.csv out1.csv   \n",
       "7   1000  17112  17109                                     cat in1.csv   \n",
       "9   1000  17112  17109                                     cat in1.csv   \n",
       "11  1000  17114  17109  python concat_csvs.py in1.csv in2.csv out3.csv   \n",
       "13  1000  17114  17109  python concat_csvs.py in1.csv in2.csv out3.csv   \n",
       "16  1000  17114  17109  python concat_csvs.py in1.csv in2.csv out3.csv   \n",
       "19  1000  17121  17109                                    cat out3.csv   \n",
       "21  1000  17121  17109                                    cat out3.csv   "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logs_df = pd.read_csv('loggedfs_events4.log')\n",
    "cols = [col for col in logs_df.columns if col in ['Action','Time','Path','PID','PPID','UID','Command Line']]\n",
    "df = logs_df[cols]\n",
    "df[(df['Action'] == 'read') | (df['Action'] == 'write')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Neo4j lineage graph from instrumentation logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lineage_graph as lg\n",
    "\n",
    "graph = lg.build_lineage_graph('loggedfs_events4.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom visualization of lineage graph using Vis.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"figure/graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1133f09b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.vis import draw\n",
    "\n",
    "options = {\"Dataset\": \"name\", \"Job\": \"uid\"}\n",
    "draw(graph, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Neo4j temporal lineage graph using Cypher\n",
    "\n",
    "## Query results as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rel_id  source_id  target_id\n",
      "0       4          0          5\n",
      "1       2          0          3\n",
      "2       0          0          1\n",
      "3       1          1          2\n",
      "4       3          3          4\n",
      "5       6          5         90\n",
      "6       5          6          5\n",
      "7      55         90         91\n",
      "8      56         91         92\n"
     ]
    }
   ],
   "source": [
    "# Sanity check query.\n",
    "query = \"\"\"\n",
    "    MATCH (n)-[r]->(m)\n",
    "    RETURN id(n) AS source_id,\n",
    "           id(r) AS rel_id,\n",
    "           id(m) AS target_id\n",
    "\"\"\"\n",
    "\n",
    "results_df = pd.DataFrame(graph.data(query))\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy detection as a reachability query between 2 content similar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 path(s) from a to b:\n",
      "\n",
      "[{'b.name': '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv', 'p': (f8d6825)-[:IS_READ_BY {data_bytes:1024,timestamp:1486075410214}]->(f2bebaa)-[:WRITES_TO {data_bytes:1024,timestamp:1486075410219}]->(cb59cd9)-[:IS_READ_BY {data_bytes:1024,timestamp:1486075411282}]->(cb23575)-[:WRITES_TO {data_bytes:1024,timestamp:1486075411283}]->(`/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv`)}]\n",
      "\n",
      "Found 0 path(s) from b to a:\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    MATCH (n:Dataset)\n",
    "    WHERE (n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'\n",
    "    OR n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv')\n",
    "    RETURN id(n) as node_id\n",
    "\"\"\"\n",
    "node_ids = graph.data(query)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(a)-[r*]->(b)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from a to b:\\n' % len(paths))\n",
    "print(paths)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(b)-[r*]->(a)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from b to a:\\n' % len(paths))\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy detection as a reachability query between 2 content similar datasets (monotonically increasing edge timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 path(s) from a to b with monotonically increasing timestamps:\n",
      "\n",
      "[{'b.name': '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv', 'p': (f8d6825)-[:IS_READ_BY {data_bytes:1024,timestamp:1486075410214}]->(f2bebaa)-[:WRITES_TO {data_bytes:1024,timestamp:1486075410219}]->(cb59cd9)-[:IS_READ_BY {data_bytes:1024,timestamp:1486075411282}]->(cb23575)-[:WRITES_TO {data_bytes:1024,timestamp:1486075411283}]->(`/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv`)}]\n",
      "\n",
      "Found 0 path(s) from b to a with monotonically increasing timestamps:\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    MATCH (n:Dataset)\n",
    "    WHERE (n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv'\n",
    "    OR n.name = '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv')\n",
    "    RETURN id(n) as node_id\n",
    "\"\"\"\n",
    "node_ids = graph.data(query)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(a)-[r*]->(b)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    WHERE all(r2 in relationships(p)\n",
    "              where r2.timestamp>=r1.timestamp)    \n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from a to b with monotonically increasing timestamps:\\n' % len(paths))\n",
    "print(paths)\n",
    "\n",
    "# Path query with monotonically increasing timestamps.\n",
    "query = \"\"\"\n",
    "    START a=node(%d), b=node(%d)\n",
    "    MATCH p=(b)-[r*]->(a)\n",
    "    WITH head(relationships(p)) as r1, p, b\n",
    "    WHERE all(r2 in relationships(p)\n",
    "              where r2.timestamp>=r1.timestamp)    \n",
    "    RETURN p, b.name\n",
    "\"\"\" % (node_ids[0]['node_id'], node_ids[1]['node_id'])\n",
    "\n",
    "\n",
    "paths = graph.data(query)\n",
    "print('\\nFound %d path(s) from b to a with monotonically increasing timestamps:\\n' % len(paths))\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Aggregate Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    START from=node(*)\n",
    "    MATCH p=(from)-->(to)\n",
    "    WITH from as from, to as to, count(p) as paths\n",
    "    WHERE paths > 1\n",
    "    RETURN to,paths\n",
    "\"\"\"\n",
    "\n",
    "paths = graph.data(query)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cypher extension is already loaded. To reload it, use:\n",
      "  %reload_ext cypher\n",
      "20 rows affected.\n",
      "\n",
      "Original Nodes:\n",
      "[('6', {'name': '/home/lubuntu/src/file_access_monitor/test_workflows/in2.csv', 'labels': ['Dataset']}), ('91', {'name': 'cat out3.csv ', 'uid': 'PID=17121,CLI=cat out3.csv ', 'pid': 17121, 'labels': ['Job']}), ('0', {'name': '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv', 'labels': ['Dataset']}), ('4', {'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out2.csv', 'labels': ['Dataset']}), ('90', {'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out3.csv', 'labels': ['Dataset']}), ('1', {'name': 'cp in1.csv out1.csv ', 'uid': 'PID=17110,CLI=cp in1.csv out1.csv ', 'pid': 17110, 'labels': ['Job']}), ('5', {'name': 'python concat_csvs.py in1.csv in2.csv out3.csv ', 'uid': 'PID=17114,CLI=python concat_csvs.py in1.csv in2.csv out3.csv ', 'pid': 17114, 'labels': ['Job']}), ('2', {'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out1.csv', 'labels': ['Dataset']}), ('92', {'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv', 'labels': ['Dataset']}), ('3', {'name': 'cat in1.csv ', 'uid': 'PID=17112,CLI=cat in1.csv ', 'pid': 17112, 'labels': ['Job']})]\n",
      "node=6, data={'name': '/home/lubuntu/src/file_access_monitor/test_workflows/in2.csv', 'labels': ['Dataset']}\n",
      "node=91, data={'name': 'cat out3.csv ', 'uid': 'PID=17121,CLI=cat out3.csv ', 'pid': 17121, 'labels': ['Job']}\n",
      "node=0, data={'name': '/home/lubuntu/src/file_access_monitor/test_workflows/in1.csv', 'labels': ['Dataset']}\n",
      "node=4, data={'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out2.csv', 'labels': ['Dataset']}\n",
      "node=90, data={'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out3.csv', 'labels': ['Dataset']}\n",
      "node=1, data={'name': 'cp in1.csv out1.csv ', 'uid': 'PID=17110,CLI=cp in1.csv out1.csv ', 'pid': 17110, 'labels': ['Job']}\n",
      "node=5, data={'name': 'python concat_csvs.py in1.csv in2.csv out3.csv ', 'uid': 'PID=17114,CLI=python concat_csvs.py in1.csv in2.csv out3.csv ', 'pid': 17114, 'labels': ['Job']}\n",
      "node=2, data={'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out1.csv', 'labels': ['Dataset']}\n",
      "node=92, data={'name': '/home/lubuntu/src/file_access_monitor/test_workflows/out4.csv', 'labels': ['Dataset']}\n",
      "node=3, data={'name': 'cat in1.csv ', 'uid': 'PID=17112,CLI=cat in1.csv ', 'pid': 17112, 'labels': ['Job']}\n",
      "\n",
      "Personalize Vector:\n",
      "{'2': 0, '1': 17110, '3': 17112, '0': 0, '4': 0, '90': 0, '5': 17114, '6': 0, '92': 0, '91': 17121}\n",
      "\n",
      "Original Edges:\n",
      "src=6, dst=5, edge={'timestamp': 1486075410215, 'type': 'IS_READ_BY', 'data_bytes': 1024}\n",
      "src=91, dst=92, edge={'timestamp': 1486075411283, 'type': 'WRITES_TO', 'data_bytes': 1024}\n",
      "src=0, dst=1, edge={'timestamp': 1486075407888, 'type': 'IS_READ_BY', 'data_bytes': 1024}\n",
      "src=0, dst=5, edge={'timestamp': 1486075410214, 'type': 'IS_READ_BY', 'data_bytes': 1024}\n",
      "src=0, dst=3, edge={'timestamp': 1486075408892, 'type': 'IS_READ_BY', 'data_bytes': 1024}\n",
      "src=90, dst=91, edge={'timestamp': 1486075411282, 'type': 'IS_READ_BY', 'data_bytes': 1024}\n",
      "src=1, dst=2, edge={'timestamp': 1486075407888, 'type': 'WRITES_TO', 'data_bytes': 1024}\n",
      "src=5, dst=90, edge={'timestamp': 1486075410219, 'type': 'WRITES_TO', 'data_bytes': 1024}\n",
      "src=3, dst=4, edge={'timestamp': 1486075408892, 'type': 'WRITES_TO', 'data_bytes': 1024}\n",
      "\n",
      "New Edges:\n",
      "[('2', '1', {'weight': 1024}), ('4', '3', {'weight': 1024}), ('3', '0', {'weight': 1024}), ('0', '1', {'weight': 1024}), ('0', '5', {'weight': 1024}), ('90', '5', {'weight': 1024}), ('90', '91', {'weight': 1024}), ('5', '6', {'weight': 1024}), ('92', '91', {'weight': 1024})]\n",
      "\n",
      "New Nodes:\n",
      "[('2', {}), ('4', {}), ('3', {}), ('0', {}), ('90', {}), ('1', {}), ('5', {}), ('6', {}), ('92', {}), ('91', {})]\n",
      "\n",
      "PageRank:\n",
      "{'6': 0.04594451880251435, '1': 0.12693326076228845, '91': 0.12450942095308155, '0': 0.15384162297404927, '2': 0.05394709417342418, '90': 0.09886148111510312, '5': 0.16215550738258272, '4': 0.05395000999811075, '92': 0.052916962312588765, '3': 0.1269401215262568}\n"
     ]
    }
   ],
   "source": [
    "%load_ext cypher\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "\n",
    "results = %cypher MATCH p = (a)-[r*]->(b) RETURN p\n",
    "\n",
    "# Networkx graph.\n",
    "g = results.get_graph()\n",
    "\n",
    "#nx.draw(g)\n",
    "#g.nodes(data=True)\n",
    "\n",
    "# Print nodes so that we can see their original ids and properties.\n",
    "print(\"\\nOriginal Nodes:\")\n",
    "print(g.nodes(data=True))\n",
    "\n",
    "# Node weights for personalized pagerank.\n",
    "personalize = {}\n",
    "for node, data in g.nodes(data=True):\n",
    "    if 'Dataset' in data['labels']:\n",
    "        # FIXME: 0 weight for dataset nodes?  Should we use some other attribute instead?\n",
    "        personalize[node] = 0\n",
    "    elif 'Job' in data['labels']:\n",
    "        # TODO: Use some resource from telemetry data here, e.g., CPU consumption.\n",
    "        personalize[node] = data['pid']\n",
    "    print(\"node=%s, data=%s\" % (node, data))\n",
    "\n",
    "print(\"\\nPersonalize Vector:\")\n",
    "print(personalize)\n",
    "\n",
    "print(\"\\nOriginal Edges:\")\n",
    "\n",
    "# Transformation from MultiDigraph to Graph for Pagerank calculation.\n",
    "H = nx.Graph()\n",
    "for src, dst, edge in g.edges(data=True):\n",
    "    print(\"src=%s, dst=%s, edge=%s\" % (src, dst, edge))\n",
    "    # Let's weight each edge by the amount of bytes read / written.\n",
    "    w = edge['data_bytes']\n",
    "    if H.has_edge(src, dst):\n",
    "        H[src][dst]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(src, dst, weight=w)\n",
    "\n",
    "print(\"\\nPageRank:\")\n",
    "print(nx.pagerank(H)\n",
    "print(\"\\nPersonalized PageRank:\")\n",
    "print(nx.pagerank(H, personalization=personalize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
